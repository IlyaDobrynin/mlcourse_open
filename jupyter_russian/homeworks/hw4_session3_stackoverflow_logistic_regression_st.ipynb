{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 3\n",
    "<center> Автор материала: Павел Нестеров (@mephistopheies).\n",
    "\n",
    "Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/100c3Ek94UL-VRwXrN4lxCSnGjfJrl6Gc96G21DNCh4w).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн-версию алгоритма multilabel-классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн-моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно.\n",
    "\n",
    "PS2:\n",
    "- в процессе решения домашней работы вам придется работать с текстом, и у вас может возникнуть желание сделать очевидный препроцессинг, например привести все слова в нижний регистр, в-общем **этого делать не нужно, если не оговорено заранее в задании**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install watermark\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем версии используемых библиотек. Совпадут ли ответы в случае других версий - не гарантируется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.6.4\n",
      "IPython 6.2.1\n",
      "\n",
      "numpy 1.14.0\n",
      "scipy 1.0.0\n",
      "pandas 0.22.0\n",
      "matplotlib 2.1.2\n",
      "sklearn 0.19.1\n",
      "\n",
      "compiler   : GCC 7.2.0\n",
      "system     : Linux\n",
      "release    : 4.4.0-116-generic\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 6\n",
      "interpreter: 64bit\n",
      "Git hash   :\n"
     ]
    }
   ],
   "source": [
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,sklearn -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = '../../data/stackoverflow_sample_125k.tsv'\n",
    "TAGS_FILE_NAME = '../../data/top10_tags.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ios', 'javascript', 'html', 'c#', 'python', 'java', 'c++', 'android', 'php', 'jquery'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "top_tags = set(top_tags)\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\textbf{x}\\right) &=& \\dfrac{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\textbf{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} = \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\textbf{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\textbf{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\textbf{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x_i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\textbf{x}\\right) &=& \\dfrac{p\\left(\\textbf{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\textbf{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} = \\sigma_k\\left(\\textbf{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\textbf{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x_i$ – это выражение моделируется линейной функцией от признаков объекта и параметров модели для класса $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\textbf{x}, y}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\textbf{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\textbf{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания; если предпочитаете текст, то и он есть [тут](https://www.ics.uci.edu/~pjsadows/notes.pdf) и [тут](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/)):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\textbf{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\textbf{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "<font color=\"red\">Вопрос 1.</font> Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\textbf{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "<font color=\"red\">Вопрос 2.</font> В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$. Какой вид она будет иметь?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Реализация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$, если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. В нашем случае, чтобы не пересчитывать [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model) самим или с помощью [sklearn.feature_extraction.text.CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer), мы будем идти по словам предложения в порядке их следования. Если какое-то слово встречается несколько раз, то мы добавляем его в аккумулятор со своим весом. В итоге получится то же самое, как если сначала посчитать количество одинаковых слов и домножить на соответствующий вес. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку и реализовать $\\sigma$ без риска overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # инициализируем z\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma = 1/(1 - np.exp(-z)) if z > 0 else np.exp(z)/(np.exp(z) +1)\n",
    "                    if sigma < tolerance:\n",
    "                        sigma = tolerance\n",
    "                    elif sigma > (1 - tolerance):\n",
    "                        sigma = (1 - tolerance)\n",
    "                        \n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sample_loss -= y*np.log(sigma) + (1 - y)*np.log(1-sigma)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = y - sigma\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74416e2b76034ad98c99cb861f4eaeba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=125000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 тысяч примеров, чтобы хоть как-то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7bb003f0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 24.74\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 3.</font>\n",
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. 19.74\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 4.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(\\textbf W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(\\textbf W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической корректности мы используем грязный трюк: будем регуляризировать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение (bias) не регуляризируется. `sample_loss` тоже должен остаться без изменений.\n",
    "\n",
    "Замечание:\n",
    "- не забудьте, что нужно учитывать регуляризацию слова в градиентном шаге только один раз\n",
    "- условимся, что учитываем регуляризацию только при первой встрече слова\n",
    "- если бы мы считали сначала bag-of-words, то мы бы в цикле шли по уникальным словам, но т.к. мы этого не делаем, приходится выкручиваться (еще одна жертва богу online-моделей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 5.</font> К какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(\\textbf W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(\\textbf W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(\\textbf W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Регуляризация ElasticNet , реализация\n",
    "\n",
    "В качестве седьмой задачи вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной `ElasticNet`-регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 7.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность появления каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model._vocab_inv = dict([(v, k) for (k, v) in model._vocab.items()])\n",
    "\n",
    "for tag in model._tags:\n",
    "    print(tag, ':', ', '.join([model._vocab_inv[k] for (k, v) in \n",
    "                               sorted(model._w[tag].items(), \n",
    "                                      key=lambda t: t[1], \n",
    "                                      reverse=True)[:5]]))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 8.</font> Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что не удивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. c# \n",
    "2. javascript\n",
    "3. jquery\n",
    "4. android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре – 519290, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов, используя данные из ``train``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Вопрос 9.</font> К какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение этого задания вам предлагается реализовать метод `predict_proba`, который принимает строку, содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty \" + \n",
    "            \"level\").lower().replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(model.predict_proba(sentence).items(), \n",
    "       key=lambda t: t[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Вопрос 10.</font> Отметьте все теги, ассоциирующиеся с данным вопросом, если порог принятия равен $0.9$. То есть считаем, что вопросу надо поставить некоторый тег, если вероятность его появления, предсказанная моделью, больше или равна 0.9. \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. php\n",
    "4. java"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
